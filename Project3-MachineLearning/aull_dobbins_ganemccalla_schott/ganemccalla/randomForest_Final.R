#we import data
train_raw = read.csv('train_total.csv', header=TRUE)
test_raw = read.csv('test_total.csv', header=TRUE)
raions = read.csv("raion_clusters.csv")

#select a subset of it (this can vary, I've tried multiple subsets)
#we performance the same functions for both test and train data (besides selecting
#price_doc in train but no test)
reducedTrainData = dplyr::select(train_raw,price_doc,timestamp,full_sq,life_sq,
                                 floor,max_floor,material,build_year,num_room,
                                 kitch_sq,state,product_type,sub_area,kremlin_km,
                                 metro_km_walk,radiation_km,basketball_km,
                                 school_km, park_km, mkad_km, water_km,quarter)

reducedTrainData$sub_area = as.factor(reducedTrainData$sub_area)
reducedTrainData$state = as.factor(reducedTrainData$state)
reducedTrainData$material = as.factor(reducedTrainData$material)
reducedTrainData$timestamp = as.numeric(as.POSIXct(reducedTrainData$timestamp,origin="1970-01-01"))
#this matches the sub_area to the corresponding cluster
reducedTrainData$raion_cluster = as.factor(raions[match(reducedTrainData$sub_area,raions$sub_area),2])
reducedTrainData = dplyr::select(reducedTrainData,-sub_area)
reducedTestData = dplyr::select(test_raw,timestamp,full_sq,life_sq,
                                floor,max_floor,material,build_year,num_room,
                                kitch_sq,state,product_type,sub_area,kremlin_km,
                                metro_km_walk,radiation_km,basketball_km,
                                school_km, park_km, mkad_km, water_km,quarter)

reducedTestData$sub_area = as.factor(reducedTestData$sub_area)
reducedTestData$state = as.factor(reducedTestData$state)
reducedTestData$material = as.factor(reducedTestData$material)
reducedTestData$timestamp = as.numeric(as.POSIXct(reducedTestData$timestamp,origin="1970-01-01"))
reducedTestData$raion_cluster = as.factor(raions[match(reducedTestData$sub_area,raions$sub_area),2])
reducedTestData = dplyr::select(reducedTestData,-sub_area)

#this will take a while to execute, 10-30 minutes
library(VIM)
      inputedTrain = kNN(reducedTrainData, k = 9)

  inputedTest = kNN(reducedTestData, k = 9)

#this eliminates the additional columns generated by inputation
inputedTest = inputedTest[1:16]
inputedTrain = inputedTrain[1:17]
#somehow there's an empty string factor as a product type
inputedTest[inputedTest==""] = "Investment"
inputedTest$product_type <- droplevels(inputedTest$product_type)
library(randomForest)
#this will take a while to execute, at least 30 minutes
library(doMC)
# register 4 cores to be used
registerDoMC(4)

rf.base = foreach(ntree=rep(166, 3), .combine=combine, .multicombine = TRUE,
                  .packages='randomForest') %dopar% {
                    randomForest(price_doc ~ ., data = inputedTrain)
                  }
#this gives us our prediction
prediction = predict(rf.base,inputedTest)
#write the prediction, if you modify the headers and add the id's
#you can submit to kaggle
write.csv(prediction,"rfPrediction.csv")
rf.base

#this generates figures we use in the slides
svg('importance.svg')
importance(rf.base)
dev.off()
svg('varImpPlot.svg')
varImpPlot(rf.base)
dev.off()

#this takes a very long time to run, it's used in the slides
#to determine the optimal number of variables
oob.err = numeric(16)
for (mtry in 1:16) {
    fit = randomForest(price_doc ~ ., data = inputedTrain, mtry=mtry)
    oob.err[mtry] = fit$mse[500]
  cat("We're performing iteration", mtry, "\n")
}

svg('OOBError.svg')
plot(1:16, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")
dev.off()



library(forestFloor)
#these give us a color coded plot of each of the variables performances
ff = forestFloor(rf.base,inputedTrain)
Col = fcol(ff,cols=1,outlier.lim = 2.5)
plot(ff,col=Col,plot_GOF =T)