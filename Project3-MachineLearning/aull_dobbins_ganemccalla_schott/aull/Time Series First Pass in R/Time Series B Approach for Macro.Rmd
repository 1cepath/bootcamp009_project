---
title: "Time Series Approach for Macro"
author: "Wes Aull"
date: "5/18/2017"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(psych)
library(reshape2)
library(MASS)
library(lubridate)
library(xts)
library(ggplot2)
library(ggthemes)
library(readr)
library(corrplot)
library(caret)
library(car)
library(fastICA)
library(MTS)
library(stats)
library(xts)
library(zoo)
library(KFAS)
library(dlmodeler)
library(vars)
library(dplyr)
library(tseries)
library(prophet)
library(dse)
library(midasr)
library(corrplot)
library(forecast)
library(TTR)
library(caret)
library(caTools)
library(quantmod)

rice$p_sqf
############## Predefined functions #############
ma <- function(x,n=5){filter(x,rep(1/n,n), sides=1)}


############### Import Macro Data and Training Data To Construct Dependent Variable ####################

macro <- read_csv("./macro.csv", 
    col_types = cols(apartment_fund_sqm = col_number(), 
        average_provision_of_build_contract = col_number(), 
        average_provision_of_build_contract_moscow = col_number(), 
        balance_trade_growth = col_number(), 
        bandwidth_sports = col_number(), 
        deposits_rate = col_number(), gdp_deflator = col_number(), 
        gdp_quart = col_number(), gdp_quart_growth = col_number(), 
        grp_growth = col_number(), hospital_bed_occupancy_per_year = col_number(), 
        hospital_beds_available_per_cap = col_number(), 
        incidence_population = col_number(), 
        modern_education_share = col_number(), 
        mortgage_growth = col_number(), net_capital_export = col_number(), 
        oil_urals = col_number(), old_education_build_share = col_number(), 
        population_reg_sports_share = col_number(), 
        provision_retail_space_sqm = col_number(), 
        rent_price_1room_bus = col_number(), 
        rent_price_1room_eco = col_number(), 
        rent_price_2room_bus = col_number(), 
        rent_price_2room_eco = col_number(), 
        rent_price_3room_bus = col_number(), 
        rent_price_3room_eco = col_number(), 
        `rent_price_4+room_bus` = col_number(), 
        salary_growth = col_number(), students_state_oneshift = col_number(), 
        timestamp = col_date(format = "%Y-%m-%d")))


problems(macro) #0 rows - No data corruption classes were assigned.

macro$rent_price_2room_eco <- ifelse(macro$rent_price_2room_eco == .1, 40.25, macro$rent_price_2room_eco)

macro$rent_price_1room_eco <- ifelse(macro$rent_price_1room_eco == 2.31, 32.61, macro$rent_price_1room_eco)


for (i in seq(ncol(macro),2)) {
  macro[,i] = as.numeric(unlist(macro[,i]))
}

train <- read_csv("./train_clean.csv", 
    col_types = cols(build_year = col_number(), 
        max_floor = col_number(), timestamp = col_date(format = "%Y-%m-%d")))

## Join the data and mutate the dependent variable to be modeled (price / sq. foot).

price = data.frame(train$timestamp,train$price_doc,train$full_sq)
colnames(price) = c('timestamp','price_doc','full_sq')

# Log transform (Leaving it turned off right now while doing EDA).
#price$price_doc = log(price$price_doc)
#price$full_sq = log(price$full_sq)

#Re-frame data for better features to model at the Russian macro level.

price = price %>%
  mutate(p_sqf = price_doc / full_sq)
price$price_doc = NULL
price$full_sq = NULL

macro = macro %>%
  mutate(brent_rub = brent * usdrub)
macro_brent = NULL

## Aggregate duplicate observations for one date due to time series analysis limitation.
price = aggregate(x=p_sqf, by = list(unique.timestamp = price$timestamp), FUN=mean, na.rm=TRUE)

## Take fifteen day moving average.
price$p_sqf = runmean(price$p_sqf,15)
macro$brent_rub = runmean(macro$brent_rub,15)

## Calculate the rates of change based on estimated lag factors.


rm(train)
macro = left_join(price,macro, by = 'timestamp')
rm(price)

a = lm(p_sqf ~ brent_rub,macro)
plot(a)
summary(a)

## Import the training dat for sake of price and square foot data.
## Will make price / sq. foot into the dependent variable to be forecasted by the time series model,
## removing the single most important variable that explains price variance at the property level.



a = lm()

#################### Clean Data of Highly Sparse Variables & Pare Down to Complete Obs. ######################

## Remove highly sparse variables.
data = macro
for (i in seq(ncol(data),1)) {
  if (sum(is.na(data[,i])) > 400) {
    data[,i] = NULL
  }  
}

## Remove variables with no variance (no explanatory power).
data$baths_share = NULL
data$water_pipes_share = NULL
data$childbirth = NULL
data$heating_share = NULL
data$hot_water_share = NULL
data$old_house_share = NULL

## Standardize data.
preProcessParameters = preProcess(data, method = c('center','scale'))
stand_data = predict(preProcessParameters,data)

## Pare down to complete observations.
complete_stand_data = stand_data[complete.cases(stand_data),]
complete_stand_data$unique.timestamp = NULL

## Anytime a series offers a new value (besides the initial), it is kept.
## Anytime it doesn't change, the value is changed to NA.  This solves
## this issue that many different series frequences have repeated 
## observations for an observation that is not unique (and thus its
## variance is falsely reduced). 
for (j in seq((ncol(complete_stand_data)-1),1)) {
  for (i in seq(nrow(complete_stand_data),2)) {
    if (length(unique(complete_stand_data[,j])) < .5 * (length(complete_stand_data[,j]))) {
      if (complete_stand_data[i,j] == complete_stand_data[(i-1),j]) {
        complete_stand_data[i,j] = NA
      }
    }
  }
}

## Break data series also into dataframes with same frequency.  Reduce collinearities.
dly_complete_stand_data = complete_stand_data
for (j in seq((ncol(dly_complete_stand_data)-1),2)) {
  if (length(unique(dly_complete_stand_data[,j])) < .5 * (length(dly_complete_stand_data[,j]))) {
    dly_complete_stand_data[,j] = NULL
  }
}
dly_complete_stand_data = dly_complete_stand_data[complete.cases(dly_complete_stand_data),]
preProcessParameters = preProcess(dly_complete_stand_data[1:9], method = c('pca'))
dly_complete_stand_data = predict(preProcessParameters,dly_complete_stand_data)
dly_complete_stand_data$timestamp = as.POSIXlt.Date(dly_complete_stand_data$timestamp)
dly_complete_stand_data$unique.timestamp = NULL
cor(dly_complete_stand_data[,2:4])

qtr_complete_stand_data = complete_stand_data
for (j in seq((ncol(qtr_complete_stand_data)-1),2)) {
  if (length(unique(qtr_complete_stand_data[,j])) < 8 | length(unique(qtr_complete_stand_data[,j])) > 20) {
    qtr_complete_stand_data[,j] = NULL
  }
}
qtr_complete_stand_data = qtr_complete_stand_data[complete.cases(qtr_complete_stand_data),]
qtr_complete_stand_data$timestamp = as.POSIXlt.Date(qtr_complete_stand_data$timestamp)
qtr_complete_stand_data$unique.timestamp = NULL

mon_complete_stand_data = complete_stand_data
for (j in seq((ncol(mon_complete_stand_data)-1),2)) {
  if (length(unique(mon_complete_stand_data[,j])) < 20 | length(unique(mon_complete_stand_data[,j])) > 50) {
    mon_complete_stand_data[,j] = NULL
  }
}

mon_complete_stand_data = mon_complete_stand_data[complete.cases(mon_complete_stand_data),]
mon_complete_stand_data$timestamp = as.POSIXlt.Date(mon_complete_stand_data$timestamp)
mon_complete_stand_data$unique.timestamp = NULL

## Convert date to POSIXlt for entire data set.
complete_stand_data$timestamp = as.POSIXlt.Date(complete_stand_data$timestamp)

## Create a dataframe that can be used for the univariate time series analysis of price / square foot.

psq = data.frame(macro$timestamp,macro$p_sqf)
colnames(psq) = c('ds','y')
preProcessParameters = preProcess(psq, method = c('center','scale'))
stand_psq = predict(preProcessParameters,psq)
stand_psq$ds = as.POSIXct.Date(stand_psq$ds)
stand_psq$unique.timestamp = NULL

## Create complete monthly data with daily taken at snapshot.
mo_data = complete_stand_data
for (i in seq(ncol(mo_data),1)) {
  if (sum(is.na(mo_data[,i])) > 1000) {
    mo_data[,i] = NULL
  }  
}
complete_mo_data = mo_data[complete.cases(mo_data[2:ncol(mo_data)]),]
preProcessParameters = preProcess(complete_mo_data[2:29], method = c('pca'))
pca_complete_mo_data = predict(preProcessParameters,complete_mo_data[2:29])
pca_complete_mo_data = data.frame(pca_complete_mo_data,complete_mo_data[,30])
names(pca_complete_mo_data)[names(pca_complete_mo_data)=="complete_mo_data...30."] <- "p_sq"

preProcessParameters = preProcess(complete_mo_data[,2:30], method = c('YeoJohnson'))
tran_complete_mo_data = predict(preProcessParameters,complete_mo_data[,2:30])

macro_regression = lm(p_sq ~.,pca_complete_mo_data)
summary(macro_regression)
plot(macro_regression)

macro_regression = lm(p_sqf ~ mortgage_growth + deposits_growth,tran_complete_mo_data)
summary(macro_regression)
plot(macro_regression)

model.empty = lm(p_sqf ~ 1, data = tran_complete_mo_data) #The model with an intercept ONLY.
model.full = lm(p_sqf ~ ., data = tran_complete_mo_data) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))

forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)

par(mfrow=c(4,2))
par(mar = rep(2, 4))
multi.hist(tran_complete_mo_data,bcol = 'lightsalmon')

```


```{r}
macro_xts <- xts(complete_stand_data[,-1], order.by=complete_stand_data[,1])
dly_xts <- xts(dly_complete_stand_data[,-1], order.by=dly_complete_stand_data[,1])

dly_ts = as.ts(macro[,1:2])
dly_time_series_components = decompose(dly_ts)
dly_ts

## Price / Square foot is shown to be stationary process.  I also plot its univariate time series model.
psq_xts = xts(stand_psq[,-1], order.by = stand_psq[,1])
psq_mo_xts = to.monthly(psq_xts)
psq_mo_xts$psq_mo_xts.Open = NULL
psq_mo_xts$psq_mo_xts.High = NULL
psq_mo_xts$psq_mo_xts.Low = NULL
attr(psq_mo_xts, 'frequency') <- 12
a = prophet(stand_psq)
pred = predict(a,stand_psq)
plot(a, pred, fill = y) + xlab('Date') + ylab('Standardized Price / Square Foot') + theme_economist() 

# Create monthly rolled daily to monthly.
mo_xts = to.monthly(dly_xts)
mo_xts$psq_mo_xts.Open = NULL
mo_xts$psq_mo_xts.High = NULL
mo_xts$psq_mo_xts.Low = NULL
attr(mo_xts, 'frequency') <- 12

dly_ts = as.ts(psq_mo_xts[,1])
plot(decompose(dly_ts),col = 'red', col.lab = 'blue')
axis(side = 1, at = c('Jan', 'Apr', 'Jul', 'Oct'), labels = c('Jan', 'Apr', 'Jul', 'Oct'))

# Auto Arima
auto.arima(dly_complete_stand_data[,2],xreg=dly_complete_stand_data[,3:6])

#Garch - Generalized Auto-regressive Conditioned Heteroskedasticity for the univariate on price / sq.
garch(psq_xts, grad = "numerical")

# Constructing a TSdata time series object for VAR on the daily time series.
VARselect(pca_complete_mo_data[,14], lag.max = , type = "both", exogen = dly_complete_stand_data[,2:13])
# We choose a lag of 13 as having the best AIC and FPE information criteria.  A lag of 1-2 days
# also showed an excellent information criteria.
var_13_dly = VAR([,2], p = 13, type = "both", exogen=dly_complete_stand_data[,3:6])
options(max.print=1000000)
summary(var_13_dly)

# Constructing a TSdata time series object for VAR on the monthly time series.
VARselect(pca_complete_mo_data[,14], lag.max = 3, type = "both", exogen = complete_mo_data[,2:14])
# We choose a lag of 13 as having the best AIC and FPE information criteria.  A lag of 1-2 days
# also showed an excellent information criteria.
var_13_dly = VAR(pca_complete_mo_data[,2:14], p = 2, type = "both")
options(max.print=1000000)
summary(var_13_dly)



# Constructing a generalized dynamical linear model using dlmodeler.  State space
# is constructed, filtering and smoothing is applied.

```


```{r}



# ARIMA (Auto-regressive integrated moving average)
# MA(3)
psq.ma = arima(psq_xts, order = c(0,0,3))
psq.ma
# ARMA(1,1)
psq.arma = arima(psq_xts, order = c(1,0,1))
# ARIMA(2,1,2)
# This model is composed of 2 prior periods of AP, 2 prior periods of white noise
# and a first order difference.
psq.arima = arima(psq_xts, order = c(2,1,2))
```


```{r}
# MIDAS Regression
y = diff(complete_stand_data$p_sqf)
x = diff(complete_stand_data$usdrub)
trend = 1:length(y)
midas_m = weights_table(y~trend+fmls(x, 200:1015, 200))

weights_table(p_sqf ~ gdp_quart + usdrub + brent + micex_rgbi_tr, m_ts)
select_and_forecast()


##The parameter function
theta.h0 <- function(p, dk) {
   i <- (1:dk-1)/100
   pol <- p[3]*i + p[4]*i^2
   (p[1] + p[2]*i)*exp(pol)
}

##Generate coefficients
theta0 <- theta.h0(c(-0.1,10,-10,-10),4*12)

##Plot the coefficients
plot(theta0)

##Generate the predictor variable
x <- simplearma.sim(list(ar=0.6),1500*12,1,12)

##Simulate the response variable
y <- midas.sim(500,theta0,x,1)
```


```{r}
dly_ts = as.ts(psq_mo_xts[,1])
plot(decompose(dly_ts),col = 'red', col.lab = 'blue')
axis(side = 1, at = c('Jan', 'Apr', 'Jul', 'Oct'), labels = c('Jan', 'Apr', 'Jul', 'Oct'))
```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```

