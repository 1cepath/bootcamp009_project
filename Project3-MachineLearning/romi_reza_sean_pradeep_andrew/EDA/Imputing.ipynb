{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on features to choose:\n",
    "\n",
    "### From Kaggle Kernels:\n",
    "#### Extensive Sberbank Exploratory Analysis:\n",
    "**source:** https://www.kaggle.com/captcalculator/a-very-extensive-sberbank-exploratory-analysis/comments#latest-185178\n",
    "\n",
    "* 37 observations where life_sq is greater than full_sq.\n",
    "* A vast majority of the apartments have three rooms or less.\n",
    "* Look to see how life sq ~ price changes based on sub-area or distance to kremlin\n",
    "* Home price does seem to increase with population density.\n",
    "* There does not appear to be a relationship between the mean home price in a district and the districtâ€™s share of working age population.\n",
    "* Surprisingly, there is little to no correlation between price and the school variables. The school variables however are highly correlated with each other, indicating that we would not want to use all of them in a linear regression model due to multicollinearity.\n",
    "* homes with >3 top 20 universities show signs of correlation, but only one house fits that description.\n",
    "* raions that have a top 25 cultural object have a median home sale price that is higher by 1.2 million (using this feature as a factor)\n",
    "* strong positive correlation between sport_objects_raion and price_doc\n",
    "\n",
    "#### Simple Exploration Notebook - Sberbank\n",
    "**source:** https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-sberbank/comments/notebook\n",
    "\n",
    "* overall increasing trend in price as floor_num increases. A sudden increase in the house price is also observed at floor 18.\n",
    "* Individual houses seems to be costlier, check price of 0 floor houses.\n",
    "\n",
    "#### Map visualizations with external shapefile\n",
    "**source:** https://www.kaggle.com/jtremoureux/map-visualizations-with-external-shapefile/notebook\n",
    "\n",
    "* replace regions with map shapefile coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Price by Region**\n",
    "<img src=\"price_by_region.png\" width=\"550\"/> \n",
    "**Price by Region (city center)**\n",
    "<img src=\"center_transacation_prices.png\" width=\"550\"/>\n",
    "\n",
    "\n",
    "**Average Price per Region**\n",
    "<img src=\"avg_price_per_region.png\", width=\"550\">\n",
    "**Average Price Squared per Region**\n",
    "<img src=\"avg_price_squared_per_region.png\", width=\"550\">\n",
    "**Okrugs**\n",
    "<img src=\"okrug.png\", width=\"550\">\n",
    "**# of Sales per Region**\n",
    "<img src=\"sales_per_region.png\", width=\"550\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider/Reminders:\n",
    "* housing market is influenced by the economy, but regions can become \"hot\" leading to overpriced properties.\n",
    "* find or create one feature for every type of measurement that a set of similar features is representing (ie. one feature/meta-feature for location, one for size of unit, etc.) \n",
    "\n",
    "### Pipeline To Do:\n",
    "* Look into kriging regression model for geospatial features\n",
    "* Plot price to gender/age/subregion.\n",
    "* subregions\n",
    "    * linear reg to predict price just with subregion and size\n",
    "    * mean/sd/var/median. \n",
    "    * see how well you can predict the sub-region by using price and size of home as features. \n",
    "    * max floors vs. price per subregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "train_df = pd.read_csv(\"../Sberbank/train.csv\", parse_dates=['timestamp'], index_col=False)\n",
    "test_df = pd.read_csv(\"../Sberbank/test.csv\", parse_dates=['timestamp'], index_col=False)\n",
    "macro_df = pd.read_csv(\"../Sberbank/macro.csv\", parse_dates=['timestamp'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log+1 and natural log for Price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['price_doc_log'] = np.log1p(train_df['price_doc'])\n",
    "train_df['price_doc_log10'] = np.log10(train_df['price_doc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square root and squared for apartment/home size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['full_sq^2'] = np.square(train_df['full_sq'])\n",
    "train_df['full_sqrt'] = np.sqrt(train_df['full_sq'])\n",
    "\n",
    "test_df['full_sq^2'] = np.square(test_df['full_sq'])\n",
    "test_df['full_sqrt'] = np.sqrt(test_df['full_sq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge training and test data set\n",
    "\n",
    "frames = [train_df, test_df]\n",
    "\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Macro to Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add macro data to train and test data\n",
    "train_df = pd.merge(train_df, macro_df, how='left', on='timestamp')\n",
    "test_df = pd.merge(test_df, macro_df, how='left', on='timestamp')\n",
    "\n",
    "# add month, day, year to train and test data\n",
    "train_df['month'] = train_df['timestamp'].dt.month\n",
    "train_df['day'] = train_df['timestamp'].dt.day\n",
    "train_df['year'] = train_df['timestamp'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gb_median = df.groupby(['timestamp'], as_index=False).median()\n",
    "\n",
    "# macro_df = pd.merge(macro_df, gb_median.loc[:, ], how='left', on='timestamp')\n",
    "\n",
    "# macro_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Scaling/Normalizing/Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale data using the mean and sd \n",
    "def rescale(feature):\n",
    "    return feature.apply(lambda x: (x - np.mean(x))/np.std(x))\n",
    "\n",
    "# # normalize data\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def normalize(feature):\n",
    "    return feature.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Rolling Statistics on Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12,center=False).mean()\n",
    "    rolstd = timeseries.rolling(window=12,center=False).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print 'Results of Dickey-Fuller Test:'\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X_full, y_full = train_df\n",
    "n_samples = X_full.shape[0]\n",
    "n_features = X_full.shape[1]\n",
    "\n",
    "# Estimate the score on the entire dataset, with no missing values\n",
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, X_full, y_full).mean()\n",
    "print(\"Score with the entire dataset = %.2f\" % score)\n",
    "\n",
    "# Add missing values in 75% of the lines\n",
    "missing_rate = 0.75\n",
    "n_missing_samples = np.floor(n_samples * missing_rate)\n",
    "missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n",
    "                                      dtype=np.bool),\n",
    "                             np.ones(n_missing_samples,\n",
    "                                     dtype=np.bool)))\n",
    "rng.shuffle(missing_samples)\n",
    "missing_features = rng.randint(0, n_features, n_missing_samples)\n",
    "\n",
    "# Estimate the score without the lines containing missing values\n",
    "X_filtered = X_full[~missing_samples, :]\n",
    "y_filtered = y_full[~missing_samples]\n",
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, X_filtered, y_filtered).mean()\n",
    "print(\"Score without the samples containing missing values = %.2f\" % score)\n",
    "\n",
    "# Estimate the score after imputation of the missing values\n",
    "X_missing = X_full.copy()\n",
    "X_missing[np.where(missing_samples)[0], missing_features] = 0\n",
    "y_missing = y_full.copy()\n",
    "estimator = Pipeline([(\"imputer\", Imputer(missing_values=0,\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"forest\", RandomForestRegressor(random_state=0,\n",
    "                                                       n_estimators=100))])\n",
    "score = cross_val_score(estimator, X_missing, y_missing).mean()\n",
    "print(\"Score after imputation of the missing values = %.2f\" % score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
