{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "### From Kaggle Kernels:\n",
    "#### Extensive Sberbank Exploratory Analysis:\n",
    "**source:** https://www.kaggle.com/captcalculator/a-very-extensive-sberbank-exploratory-analysis/comments#latest-185178\n",
    "\n",
    "* 37 observations where life_sq is greater than full_sq.\n",
    "* A vast majority of the apartments have three rooms or less.\n",
    "* Look to see how life sq ~ price changes based on sub-area or distance to kremlin\n",
    "* Home price does seem to increase with population density.\n",
    "* There does not appear to be a relationship between the mean home price in a district and the districtâ€™s share of working age population.\n",
    "* Surprisingly, there is little to no correlation between price and the school variables. The school variables however are highly correlated with each other, indicating that we would not want to use all of them in a linear regression model due to multicollinearity.\n",
    "* homes with >3 top 20 universities show signs of correlation, but only one house fits that description.\n",
    "* raions that have a top 25 cultural object have a median home sale price that is higher by 1.2 million (using this feature as a factor)\n",
    "* strong positive correlation between sport_objects_raion and price_doc\n",
    "\n",
    "#### Simple Exploration Notebook - Sberbank\n",
    "**source:** https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-sberbank/comments/notebook\n",
    "\n",
    "* overall increasing trend in price as floor_num increases. A sudden increase in the house price is also observed at floor 18.\n",
    "* Individual houses seems to be costlier, check price of 0 floor houses.\n",
    "\n",
    "#### Map visualizations with external shapefile\n",
    "**source:** https://www.kaggle.com/jtremoureux/map-visualizations-with-external-shapefile/notebook\n",
    "\n",
    "* replace regions with map shapefile coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "# load dataset\n",
    "train_df = pd.read_csv(\"../Sberbank/train.csv\", parse_dates=['timestamp'], index_col=False)\n",
    "test_df = pd.read_csv(\"../Sberbank/test.csv\", parse_dates=['timestamp'], index_col=False)\n",
    "macro_df = pd.read_csv(\"../Sberbank/macro.csv\", parse_dates=['timestamp'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [train_df, test_df]\n",
    "\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add macro data to train/test data\n",
    "\n",
    "train_df = pd.merge(train_df, macro_df, how='left', on='timestamp')\n",
    "test_df = pd.merge(test_df, macro_df, how='left', on='timestamp')\n",
    "\n",
    "df = pd.merge(df, macro_df, how='left', on='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Date/Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add month, day, year to train and test data\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "\n",
    "# Add month-year\n",
    "month_year = (df.timestamp.dt.month + df.timestamp.dt.year * 100)\n",
    "month_year_map = month_year.value_counts().to_dict()\n",
    "df['month_year'] = month_year.map(month_year_map)\n",
    "\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (df.timestamp.dt.weekofyear + df.timestamp.dt.year * 100)\n",
    "week_year_map = week_year.value_counts().to_dict()\n",
    "df['week_year'] = week_year.map(week_year_map)\n",
    "\n",
    "\n",
    "# Add month and day-of-week\n",
    "df['month'] = df.timestamp.dt.month\n",
    "df['dow'] = df.timestamp.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['price_doc_log'] = np.log1p(df['price_doc'])\n",
    "df['price_doc_log10'] = np.log10(df['price_doc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# squared and square root of full sq\n",
    "df['full_sq^2'] = np.square(df['full_sq'])\n",
    "df['full_sqrt'] = np.sqrt(df['full_sq'])\n",
    "\n",
    "# floor ratios\n",
    "df['rel_floor'] = df['floor'] / df['max_floor'].astype(float)\n",
    "df['rel_kitch_sq'] = df['kitch_sq'] / df['full_sq'].astype(float)\n",
    "\n",
    "\n",
    "# area and population density\n",
    "df['area_km'] = df['area_m'] / 1000000\n",
    "df['density'] = df['raion_popul'] / df['area_km']\n",
    "\n",
    "\n",
    "# working population\n",
    "df['work_share'] = df['work_all'] / df['raion_popul']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Lags and calculating deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lag timestamps for one month, three months, 6 months, and 12 months in df\n",
    "# Join with appropriate macro\n",
    "\n",
    "mini_macro.columns = ['one_month_lag','oil_urals_1', 'gdp_quart_1', 'cpi_1', \n",
    "                      'usdrub_1', 'micex_cbi_tr_1', 'labor_force_1']\n",
    "df['one_month_lag'] = df['timestamp'] - pd.to_timedelta('30 days')\n",
    "df = pd.merge(df, mini_macro, how = 'left', on = 'one_month_lag')\n",
    "\n",
    "# 3 month lage\n",
    "mini_macro.columns = ['three_month_lag','oil_urals_3', 'gdp_quart_3', 'cpi_3', \n",
    "                      'usdrub_3', 'micex_cbi_tr_3', 'labor_force_3']\n",
    "df['three_month_lag'] = df['timestamp'] - pd.to_timedelta('90 days')\n",
    "df = pd.merge(df, mini_macro, how = 'left', on = 'three_month_lag')\n",
    "\n",
    "# 6 month lage\n",
    "mini_macro.columns = ['six_month_lag','oil_urals_6', 'gdp_quart_6', 'cpi_6', \n",
    "                      'usdrub_6', 'micex_cbi_tr_6', 'labor_force_6']\n",
    "df['six_month_lag'] = df['timestamp'] - pd.to_timedelta('180 days')\n",
    "df = pd.merge(df, mini_macro, how = 'left', on = 'six_month_lag')\n",
    "\n",
    "# 12 month lage\n",
    "mini_macro.columns = ['twelve_month_lag','oil_urals_12', 'gdp_quart_12', 'cpi_12', \n",
    "                      'usdrub_12', 'micex_cbi_tr_12', 'labor_force_12']\n",
    "df['twelve_month_lag'] = df['timestamp'] - pd.to_timedelta('360 days')\n",
    "df = pd.merge(df, mini_macro, how = 'left', on = 'twelve_month_lag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the deltas\n",
    "df['delta_oil_1'] = (df['oil_urals'] - df['oil_urals_1']) / df['oil_urals']\n",
    "df['delta_oil_3'] = (df['oil_urals'] - df['oil_urals_3']) / df['oil_urals']\n",
    "df['delta_oil_6'] = (df['oil_urals'] - df['oil_urals_6']) / df['oil_urals']\n",
    "df['delta_oil_12'] = (df['oil_urals'] - df['oil_urals_12']) / df['oil_urals']\n",
    "\n",
    "df['delta_usdrub_1'] = (df['usdrub'] - df['usdrub_1']) / df['usdrub']\n",
    "df['delta_usdrub_3'] = (df['usdrub'] - df['usdrub_3']) / df['usdrub']\n",
    "df['delta_usdrub_6'] = (df['usdrub'] - df['usdrub_6']) / df['usdrub']\n",
    "df['delta_usdrub_12'] = (df['usdrub'] - df['usdrub_12']) / df['usdrub']\n",
    "\n",
    "df['delta_labor_force_1'] = (df['labor_force'] - df['labor_force_1']) / df['labor_force']\n",
    "df['delta_labor_force_3'] = (df['labor_force'] - df['labor_force_3']) / df['labor_force']\n",
    "df['delta_labor_force_6'] = (df['labor_force'] - df['labor_force_6']) / df['labor_force']\n",
    "df['delta_labor_force_12'] = (df['labor_force'] - df['labor_force_12']) / df['labor_force']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Imputing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning values with incorrect information \n",
    "train_df.loc[train_df['state'] == 33, 'status'] = 3\n",
    "train_df = train_df.loc[train_df['id'] != 24718] \n",
    "train_df = train_df.loc[train_df['floor'] != 0] \n",
    "\n",
    "train_df.loc[train_df['build_year'] == 20052009, 'build_year'] = 2009\n",
    "train_df.loc[train_df['build_year'] == 215, 'build_year'] = 2015\n",
    "train_df = train_df.loc[train_df['build_year'] != 4965, :]\n",
    "df.loc[df['build_year'] == 0, 'build_year'] = 2013\n",
    "df.loc[df['build_year'] == 1, 'build_year'] = 2014\n",
    "df.loc[df['build_year'] == 2, 'build_year'] = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Scaling/Normalizing/Reshaping\n",
    " (Note: not necessary for Random Forest Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(feature):\n",
    "    return feature.apply(lambda x: (x - np.mean(x))/np.std(x))\n",
    "\n",
    "def normalize(feature):\n",
    "    return feature.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check dtypes and split objects and numeric types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_numerics = df.select_dtypes(exclude=['object'])\n",
    "df_objects = df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize and rescale numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezarad/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_numerics.loc[:, df_numerics.columns != 'timestamp'] = normalize(rescale(df_numerics.select_dtypes(exclude=['datetime'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Categorical Variables to Numerics and merge again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = [df_numerics, df_objects]\n",
    "\n",
    "df = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38133, 406)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_full = df['price_doc'].values\n",
    "X_full = df.loc[:, df.columns != 'price_doc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if any feature consists of non-float value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_df_{0}.csv'.format(datetime.date(datetime.now())), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
