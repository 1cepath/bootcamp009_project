{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "```python \n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install SQLAlchemy\n",
    "```\n",
    "```bash\n",
    "pip install Cython\n",
    "```\n",
    "```bash\n",
    "pip install mysql-python\n",
    "```\n",
    "For mac users, if you get the \n",
    "\n",
    "`EnvironmentError: mysql_config not found` \n",
    "\n",
    "when install `mysql-python` please follow the solution on stackoverflow [here](https://stackoverflow.com/a/25491082)\n",
    "\n",
    "For windows users, try conda install mysql-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `CounterVectorizer` in scikit-learn package implements both tokenization and occurrence counting. The default configuration tokenizes the string by extracting words of at least 2 letters.\n",
    "- You can change it by setting the `token_pattern` parameter using regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'nlp': 2, u'r': 5, u'love': 1, u'i': 0, u'over': 3, u'python': 4}\n",
      "[[1 1 0 1 1 1]\n",
      " [1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w+\\b')\n",
    "X = vectorizer.fit_transform([\"I love Python over R.\", \"I love NLP.\"])\n",
    "print vectorizer.vocabulary_\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we are showing an example how to create a **bigram bag** of words matrix using the same `CountVectorizer` class from scikit learn.\n",
    "- The only difference is the `ngram_range` parameter. To get exact bigram, you can use `ngram_range=(2,2)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'nlp': 5, u'love': 2, u'love python': 4, u'python over': 9, u'i': 0, u'over': 6, u'python': 8, u'r': 10, u'i love': 1, u'over r': 7, u'love nlp': 3}\n",
      "[[1 1 1 0 1 0 1 1 1 1 1]\n",
      " [1 1 1 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "Y = bigram_vectorizer.fit_transform([\"I love Python over R.\", \"I love NLP.\"])\n",
    "print bigram_vectorizer.vocabulary_\n",
    "print Y.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily implement the N-gram function using pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 'this'),\n",
       " ('this', 'happened'),\n",
       " ('happened', 'more'),\n",
       " ('more', 'or'),\n",
       " ('or', 'less')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "find_ngrams(input_list, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of cooccurence matrix decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "la = np.linalg\n",
    "words = [\"I\", \"love\", \"Python\", \"over\", \"R\", \"NLP\", \".\"]\n",
    "X = np.array([[0,2,0,0,0,0,0],\n",
    "              [2,0,1,0,0,1,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,0,1,0,1,0,0],\n",
    "              [0,0,0,1,1,0,1],\n",
    "              [0,1,0,0,1,0,1],\n",
    "              [0,0,0,0,1,1,0]])\n",
    "U, s, Vh = la.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d416550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "for i in range(len(words)):\n",
    "    plt.text(U[i,0], U[i,1], words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "\n",
    "- Today, we will use a database donated from Stefan Heinz, Yvonne Lau and Daniel Epstein. They scraped the reviews from [Metacritic](http://www.metacritic.com/), including games, tv shows and movies.\n",
    "- Check out their blog post [here](https://blog.nycdatascience.com/student-works/capstone/metarecommendr-recommendation-system-video-games-movies-tv-shows/) and also their awesome [Flask app](https://github.com/Steeefan/nycdsa-proj-04).\n",
    "- The following is the structure of the database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Tables_in_capstone|\n",
    "| ------------- |\n",
    "|tblAvgRating   |\n",
    "| tblGame       |\n",
    "| tblMovie      |\n",
    "| tblReview     |\n",
    "| tblSysNice    |\n",
    "| tblTVShow     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named MySQLdb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1104f48be8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a sql engine that connects to AWS RDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mysql://bootcamp:bcstudent@metacriticdata.ckfkocwbkmmc.us-west-2.rds.amazonaws.com:3306/capstone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load all the reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM tblReview;'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/sqlalchemy/engine/__init__.pyc\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'strategy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/sqlalchemy/engine/strategies.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, name_or_url, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mdbapi_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mdbapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialect_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdbapi_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mdialect_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dbapi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.pyc\u001b[0m in \u001b[0;36mdbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MySQLdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named MySQLdb"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# Create a sql engine that connects to AWS RDS\n",
    "engine = create_engine('mysql://bootcamp:bcstudent@metacriticdata.ckfkocwbkmmc.us-west-2.rds.amazonaws.com:3306/capstone')\n",
    "# Load all the reviews\n",
    "reviews = pd.read_sql_query('SELECT * FROM tblReview;', engine)\n",
    "reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load games dataset\n",
    "games = pd.read_sql_query('SELECT * FROM tblGame;', engine)\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the uniqueID and the name columns from games so we can merge it with reviews dataframe later.\n",
    "games = games[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tv shows dataset\n",
    "tv = pd.read_sql_query(\"SELECT * FROM tblTVShow;\", engine)\n",
    "tv = tv[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load movies dataset\n",
    "movies = pd.read_sql_query(\"SELECT * FROM tblMovie;\", engine) \n",
    "movies = movies[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concate three small tables to get a full list of item names\n",
    "ids = pd.concat([games, tv, movies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, ids, how='left', on='uniqueID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build word2vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "word2vec_model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keeping the input as a Python built-in list is convenient, but can use up a lot of RAM when the input is large.\n",
    "- Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "- The `simple_preprocess` function from gensim converts a document into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class MySentences(object): \n",
    "    def __iter__(self):\n",
    "        for i in range(reviews.shape[0]):\n",
    "            yield simple_preprocess(reviews.iloc[i,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min-count` is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them. A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
    "\n",
    "- `size` is the length of your output vector of each word, which is also the number of neurons in the hidde layer.\n",
    "\n",
    "- `workers` parameter has only effect if you have Cython installed. Without Cython, you’ll only be able to use one core because of the GIL (and word2vec training will be miserably slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing word2vec model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a01d880db0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'import multiprocessing\\nimport os\\ncores = multiprocessing.cpu_count()\\n\\nif not os.path.exists(\\'models/word2vec.model\\'):\\n    print \"start traing word2vec model...\"\\n    sentences = MySentences() # a memory-friendly iterator\\n    word2vec_model = gensim.models.Word2Vec(sentences, min_count=20, size=200, workers=cores)\\n    if not os.path.exists(\\'models\\'):\\n        os.makedirs(\\'models\\')\\n        word2vec_model.save(\\'models/word2vec.model\\')\\n    else:\\n        word2vec_model.save(\\'models/word2vec.model\\')\\nelse:\\n    word2vec_model = Word2Vec.load(\\'models/word2vec.model\\')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,\n\u001b[1;32m    480\u001b[0m                        start_alpha=self.alpha, end_alpha=self.min_alpha)\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \"\"\"\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rachelkogan/anaconda/envs/py27/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cb3df93d1f04>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMySentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "import os\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "if not os.path.exists('models/word2vec.model'):\n",
    "    print \"start traing word2vec model...\"\n",
    "    sentences = MySentences() # a memory-friendly iterator\n",
    "    word2vec_model = gensim.models.Word2Vec(sentences, min_count=20, size=200, workers=cores)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        word2vec_model.save('models/word2vec.model')\n",
    "    else:\n",
    "        word2vec_model.save('models/word2vec.model')\n",
    "else:\n",
    "    word2vec_model = Word2Vec.load('models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.word_vec('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also download some pre-trained model from [here](https://github.com/3Top/word2vec-api).\n",
    "- Most of them are trained using huge corpus, so the model would be around couple gigabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a doc2vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(reviews.shape[0]):\n",
    "            yield TaggedDocument(words=simple_preprocess(reviews.iloc[i,5]), tags=['%s' % reviews.iloc[i,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing doc2vec model...\n",
      "CPU times: user 26min 40s, sys: 4min 42s, total: 31min 23s\n",
      "Wall time: 20min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists('models/doc2vec.model'):\n",
    "    print \"start traing doc2vec model...\"\n",
    "    documents = MyDocs()\n",
    "    doc2vec_model = Doc2Vec(dm=1, dbow_words=1, size=200, window=8, min_count=20, workers=cores)\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "    else:\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "else:\n",
    "    doc2vec_model = Doc2Vec.load('models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(keyword):\n",
    "    result = []\n",
    "    for name in reviews.name:\n",
    "        if keyword in name.lower():\n",
    "            result.append(name)\n",
    "    return set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search('la la land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print doc2vec_model.docvecs.most_similar('La La Land', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
