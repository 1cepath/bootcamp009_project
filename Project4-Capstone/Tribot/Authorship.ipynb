{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    " \n",
    "# Load data\n",
    "data_folder = r\"./books/\"\n",
    "files = sorted(glob.glob(os.path.join(data_folder, \"chapter*.txt\")))\n",
    "chapters = []\n",
    "for fn in files:\n",
    "    with open(fn) as f:\n",
    "        chapters.append(f.read().replace('\\n', ' '))\n",
    "all_text = ' '.join(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "print all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num, ch_text in enumerate(chapters):\n",
    "    ch_text = ch_text.decode('utf-8','ignore')\n",
    "    chapters[num] = ch_text\n",
    "all_text = all_text.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "num_chapters = len(chapters)\n",
    "fvs_lexical = np.zeros((len(chapters), 3), np.float64)\n",
    "fvs_punct = np.zeros((len(chapters), 3), np.float64)\n",
    "for e, ch_text in enumerate(chapters):\n",
    "    # note: the nltk.word_tokenize includes punctuation\n",
    "    \n",
    "\n",
    "    tokens = nltk.word_tokenize(ch_text.lower())\n",
    "    words = word_tokenizer.tokenize(ch_text.lower())\n",
    "    sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "    vocab = set(words)\n",
    "    words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                   for s in sentences])\n",
    " \n",
    "    # average number of words per sentence\n",
    "    fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "    # sentence length variation\n",
    "    fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "    # Lexical diversity\n",
    "    fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    " \n",
    "    # Commas per sentence\n",
    "    fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "    # Semicolons per sentence\n",
    "    fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "    # Colons per sentence\n",
    "    fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    " \n",
    "# apply whitening to decorrelate the features\n",
    "fvs_lexical = whiten(fvs_lexical)\n",
    "fvs_punct = whiten(fvs_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ':', u' 10-K 1 a2016form10-k.htm FORM 10-K     ')\n",
      "(1, ':', u' 10-K 1 a2015form10-k.htm FORM 10-K     ')\n",
      "(2, ':', u' 10-K 1 a2014form10-kq42014.htm FORM 10-')\n",
      "(3, ':', u' 10-K 1 a2013form10-kq42013.htm FORM 10-')\n",
      "(4, ':', u' 10-K 1 amzn-20161231x10k.htm FORM 10-K ')\n",
      "(5, ':', u' 10-K 1 amzn-20151231x10k.htm FORM 10-K ')\n",
      "(6, ':', u' 10-K 1 amzn-20141231x10k.htm FORM 10-K ')\n",
      "(7, ':', u' 10-K 1 amzn-20131231x10k.htm FORM 10-K ')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chapter in chapters:\n",
    "    print(i,':',chapter[:40])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13.13704919  19.41564851   3.40078282]\n",
      " [ 13.04144336  19.11711712   3.48684797]\n",
      " [ 13.00195036  19.08429878   3.77173869]\n",
      " [ 12.76193038  18.70033585   3.69637967]\n",
      " [ 15.08392111  17.10007293   5.49206889]\n",
      " [ 14.79174992  16.82074908   5.43014283]\n",
      " [ 14.87933335  17.04965492   5.4731785 ]\n",
      " [ 15.11225691  17.68816674   5.85598941]]\n",
      "[[ 5.10424566  5.39541748  5.20753883]\n",
      " [ 5.1632636   5.53303653  5.06911569]\n",
      " [ 5.1578059   5.44373908  4.77052986]\n",
      " [ 4.91566093  5.22923259  4.35303042]\n",
      " [ 7.26871103  6.56215711  2.7929527 ]\n",
      " [ 6.96301008  6.25905929  2.70419361]\n",
      " [ 6.99313078  6.72362715  2.94898217]\n",
      " [ 7.06733365  8.43264667  3.26638949]]\n"
     ]
    }
   ],
   "source": [
    "print(fvs_lexical)\n",
    "print(fvs_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10-K', u'1', u'a2016form10-k.htm', u'FORM', u'10-K', u'Document', u'UNITED', u'STATES', u'SECURITIES', u'AND']\n",
      "[[  0.   1.   1.   1.   0.   0.   2.   0.   0.   1.   1.   0.   0.   1.\n",
      "   13.   1.   0.   0.   0.   0.   0.   1.   0.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   9.   0.   0.   0.   0.   1.   1.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   1.   0.   0.   5.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   3.   1.   1.   0.   1.   2.   0.   0.   0.   0.   0.   0.   1.\n",
      "   14.   2.   1.   0.   0.   1.   0.   0.   0.   0.   4.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   8.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   1.   0.   0.   5.   2.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   1.   0.   3.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "   15.   2.   1.   0.   0.   1.   0.   0.   0.   0.   5.   0.   1.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.  10.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   1.   0.   0.   7.   0.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   1.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.\n",
      "   12.   1.   1.   0.   0.   0.   0.   0.   0.   0.   7.   0.   1.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.  12.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   4.   0.   0.   0.   1.   0.   0.   1.   0.\n",
      "    0.   1.   0.   0.   7.   3.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    2.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   3.   1.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  1.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    3.   1.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   3.   1.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    4.   1.   0.   0.   6.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   2.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   1.   0.   1.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [  0.   0.   1.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    3.   0.   0.   0.   8.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.   1.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# get most common words in the whole book\n",
    "NUM_TOP_WORDS = 20\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "all_tokens_lst = [nltk.word_tokenize(chapter) for chapter in chapters]\n",
    "\n",
    "print(all_tokens[:10])\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "vocab = fdist.keys()[:NUM_TOP_WORDS]\n",
    "\n",
    "fdist_lst = [nltk.FreqDist(all_tokens_chapter) for all_tokens_chapter in all_tokens_lst]\n",
    "vocab_lst = []\n",
    "\n",
    "for fdist in fdist_lst:\n",
    "    vocab = fdist.keys()[:NUM_TOP_WORDS]\n",
    "    vocab_lst.append(vocab)\n",
    "\n",
    "# print(vocab_lst)\n",
    "import itertools\n",
    "vocab = itertools.chain(*vocab_lst)\n",
    "vocab = set(vocab)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# use sklearn to create the bag for words feature vector for each chapter\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "print(vectorizer.fit_transform(chapters).toarray().astype(np.float64))\n",
    "fvs_bow = vectorizer.fit_transform(chapters).toarray().astype(np.float64)\n",
    "\n",
    "# normalise by dividing each row by its Euclidean norm\n",
    "# print(np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]==0)\n",
    "fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.05725983  0.05725983  0.05725983  0.          0.\n",
      "   0.11451967  0.          0.          0.05725983  0.05725983  0.          0.\n",
      "   0.05725983  0.74437783  0.05725983  0.          0.          0.          0.\n",
      "   0.          0.05725983  0.          0.          0.1717795   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.5153385   0.          0.          0.          0.\n",
      "   0.05725983  0.05725983  0.          0.          0.          0.          0.\n",
      "   0.          0.05725983  0.          0.          0.          0.          0.\n",
      "   0.05725983  0.          0.          0.05725983  0.          0.\n",
      "   0.28629917  0.11451967  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.1641527   0.05471757  0.05471757  0.          0.05471757\n",
      "   0.10943513  0.          0.          0.          0.          0.          0.\n",
      "   0.05471757  0.76604592  0.10943513  0.05471757  0.          0.\n",
      "   0.05471757  0.          0.          0.          0.          0.21887026\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.05471757  0.          0.          0.          0.43774052  0.          0.\n",
      "   0.          0.          0.          0.05471757  0.          0.          0.\n",
      "   0.          0.          0.          0.05471757  0.          0.          0.\n",
      "   0.          0.          0.05471757  0.          0.          0.05471757\n",
      "   0.          0.          0.27358783  0.10943513  0.05471757  0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.04845016  0.04845016  0.          0.14535047\n",
      "   0.04845016  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.72675237  0.09690032  0.04845016  0.          0.\n",
      "   0.04845016  0.          0.          0.          0.          0.24225079\n",
      "   0.          0.04845016  0.04845016  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.48450158  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.09690032  0.          0.          0.          0.          0.\n",
      "   0.          0.04845016  0.          0.          0.04845016  0.          0.\n",
      "   0.33915111  0.          0.04845016  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.04862166  0.04862166  0.          0.\n",
      "   0.04862166  0.          0.          0.04862166  0.          0.          0.\n",
      "   0.          0.58345997  0.04862166  0.04862166  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.34035165  0.\n",
      "   0.04862166  0.04862166  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.58345997  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.19448666  0.          0.          0.          0.04862166  0.          0.\n",
      "   0.04862166  0.          0.          0.04862166  0.          0.\n",
      "   0.34035165  0.14586499  0.04862166  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.14002801  0.          0.14002801  0.          0.\n",
      "   0.14002801  0.          0.          0.          0.          0.          0.\n",
      "   0.28005602  0.          0.          0.          0.14002801  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.14002801  0.          0.          0.          0.          0.\n",
      "   0.42008403  0.14002801  0.          0.14002801  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.70014004  0.          0.          0.          0.          0.\n",
      "   0.          0.28005602  0.          0.14002801  0.14002801  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.13018891  0.          0.13018891  0.          0.13018891  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.39056673  0.13018891  0.          0.          0.26037782  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.13018891  0.          0.          0.          0.          0.\n",
      "   0.39056673  0.13018891  0.          0.13018891  0.          0.          0.\n",
      "   0.          0.          0.          0.13018891  0.          0.          0.\n",
      "   0.          0.65094455  0.          0.          0.          0.          0.\n",
      "   0.          0.26037782  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.12038585  0.          0.24077171  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.48154341  0.12038585  0.          0.          0.72231512  0.          0.\n",
      "   0.          0.          0.          0.12038585  0.          0.          0.\n",
      "   0.          0.12038585  0.          0.          0.          0.          0.\n",
      "   0.24077171  0.          0.12038585  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.12038585  0.          0.12038585  0.          0.12038585\n",
      "   0.          0.          0.          0.          0.          0.12038585\n",
      "   0.        ]\n",
      " [ 0.          0.          0.10783277  0.          0.21566555  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.32349832  0.          0.          0.          0.86266219  0.          0.\n",
      "   0.          0.          0.          0.10783277  0.          0.          0.\n",
      "   0.          0.10783277  0.          0.          0.          0.          0.\n",
      "   0.10783277  0.          0.10783277  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.10783277\n",
      "   0.          0.          0.          0.          0.10783277  0.          0.\n",
      "   0.          0.10783277  0.          0.          0.          0.10783277\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(fvs_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13335.  14054.   6433.  11021.   6915.   6844.]\n",
      " [ 12918.  13369.   6249.  10596.   6636.   6588.]\n",
      " [ 11686.  12416.   5529.   9806.   6250.   6372.]\n",
      " [ 12172.  13033.   5932.  10247.   6372.   6579.]\n",
      " [  6229.   4042.   1983.   4705.   3615.   4182.]\n",
      " [  6240.   3898.   1988.   4707.   3692.   4185.]\n",
      " [  6348.   4165.   2047.   4849.   3708.   4227.]\n",
      " [  5868.   4161.   1871.   4507.   3369.   3882.]]\n"
     ]
    }
   ],
   "source": [
    "# get part of speech for each token in each chapter\n",
    "from nltk.data import load\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch)\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "chapters_pos = [token_to_pos(ch) for ch in chapters]\n",
    " \n",
    "# count frequencies for common POS types\n",
    "pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "# tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "# pos_list = tagdict.keys()\n",
    "\n",
    "fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                       for ch in chapters_pos]).astype(np.float64)\n",
    "\n",
    "print(fvs_syntax)\n",
    "# normalise by dividing each row by number of tokens in the chapter\n",
    "fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP$',\n",
       " 'VBG',\n",
       " 'VBD',\n",
       " '``',\n",
       " 'VBN',\n",
       " ',',\n",
       " \"''\",\n",
       " 'VBP',\n",
       " 'WDT',\n",
       " 'JJ',\n",
       " 'WP',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'RP',\n",
       " '$',\n",
       " 'NN',\n",
       " ')',\n",
       " '(',\n",
       " 'FW',\n",
       " 'POS',\n",
       " '.',\n",
       " 'TO',\n",
       " 'LS',\n",
       " 'RB',\n",
       " ':',\n",
       " 'NNS',\n",
       " 'NNP',\n",
       " 'VB',\n",
       " 'WRB',\n",
       " 'CC',\n",
       " 'PDT',\n",
       " 'RBS',\n",
       " 'RBR',\n",
       " 'CD',\n",
       " 'PRP',\n",
       " 'EX',\n",
       " 'IN',\n",
       " 'WP$',\n",
       " 'MD',\n",
       " 'NNPS',\n",
       " '--',\n",
       " 'JJS',\n",
       " 'JJR',\n",
       " 'SYM',\n",
       " 'UH']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "pos_list = tagdict.keys()\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRP$', 'VBG', 'VBD', '``', 'VBN', ',', \"''\", 'VBP', 'WDT', 'JJ', 'WP', 'VBZ', 'DT', 'RP', '$', 'NN', ')', '(', 'FW', 'POS', '.', 'TO', 'LS', 'RB', ':', 'NNS', 'NNP', 'VB', 'WRB', 'CC', 'PDT', 'RBS', 'RBR', 'CD', 'PRP', 'EX', 'IN', 'WP$', 'MD', 'NNPS', '--', 'JJS', 'JJR', 'SYM', 'UH']\n"
     ]
    }
   ],
   "source": [
    "print(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13690262  0.14428417  0.06604384  0.11314614  0.07099225  0.07026333]\n",
      " [ 0.13769066  0.14249779  0.06660698  0.11294088  0.07073194  0.07022032]\n",
      " [ 0.13411373  0.14249154  0.06345326  0.11253802  0.07172778  0.0731279 ]\n",
      " [ 0.13435323  0.14385686  0.06547678  0.11310529  0.07033345  0.0726183 ]\n",
      " [ 0.13751159  0.08923131  0.04377677  0.10386772  0.07980485  0.09232196]\n",
      " [ 0.13812032  0.08628093  0.04400372  0.10418788  0.08172119  0.09263358]\n",
      " [ 0.13805402  0.09057892  0.04451742  0.10545431  0.08064025  0.09192728]\n",
      " [ 0.13698438  0.09713565  0.04367719  0.10521278  0.07864696  0.09062259]]\n"
     ]
    }
   ],
   "source": [
    "print(fvs_syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=10, verbose=0)\n",
    "    km.fit(fvs)\n",
    " \n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lexical:', array([1, 1, 1, 1, 0, 0, 0, 0]))\n",
      "('Syntax:', array([0, 0, 0, 0, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print('Lexical:', PredictAuthors(fvs_lexical).labels_)\n",
    "# print('Punc:',PredictAuthors(fvs_punct).labels_)\n",
    "print('Syntax:',PredictAuthors(fvs_syntax).labels_)\n",
    "lexical_predict = PredictAuthors(fvs_lexical).labels_\n",
    "syntax_predict = PredictAuthors(fvs_syntax).labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BOW:', array([0, 0, 0, 0, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print('BOW:',PredictAuthors(fvs_bow).labels_)\n",
    "bow_predict = PredictAuthors(fvs_bow).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "true_classification = np.array([0,0,0,0,1,1,1,1])\n",
    "true_classification = 1-true_classification\n",
    "print(100*sum(lexical_predict - true_classification)/len(true_classification))\n",
    "print(100*sum(syntax_predict - true_classification)/len(true_classification))\n",
    "print(100*sum(bow_predict - true_classification)/len(true_classification))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictAuthors(fvs_syntax).cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredictAuthors(fvs_syntax).predict([[0.12593638,  0.04548909,  0.08663554,  0.09901205, 0.05591141,  0.02073608]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredictAuthors(fvs_bow).predict( [[ 0.,          0.,          0.,          0.,          0.,          0.98058068,\n",
    "   0.,          0.,          0.,          0.19611614]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredictAuthors(fvs_bow).cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "centroids = PredictAuthors(fvs_bow).cluster_centers_\n",
    "\n",
    "print(centroids)\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
