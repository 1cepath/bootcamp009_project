{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib \n",
    "ACCESS_KEY = \"xxx\"\n",
    "# Encode the Secret Key as that can contain \"/\"\n",
    "SECRET_KEY = \"xxx\".replace(\"/\", \"%2F\")\n",
    "# ENCODED_SECRET_KEY = urllib.quote(SECRET_KEY,\"\")\n",
    "AWS_BUCKET_NAME = \"project4capstones3\"\n",
    "MOUNT_NAME = \"twitter_246821242_612111\"\n",
    "#dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlContext.read.json(\"/mnt/%s/2017/06/18/03/project*\"%MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing values\n",
    "data_rm_na = data.filter(data['status_id']!='None')\n",
    "print data_rm_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_interest = ['rt_status_user_followers_count', \n",
    "                        'rt_status_user_friends_count', \n",
    "                        'rt_status_user_statuses_count', \n",
    "                        'rt_status_retweet_count',\n",
    "                        'rt_status_user_listed_count', \n",
    "                        'rt_status_user_name',\n",
    "                        'rt_status_user_profile_image',\n",
    "                        'rt_status_num_user_mentions',\n",
    "                        'rt_status_user_id', \n",
    "                        'searched_names', \n",
    "                        'rt_status_sentMag', \n",
    "                        'rt_status_sentScore',\n",
    "                        'rt_status_text', \n",
    "                        'rt_status_favorite_count']\n",
    "\n",
    "df_reduce= data_rm_na.select(features_of_interest)\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_user_followers_count\", df_reduce[\"rt_status_user_followers_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_user_friends_count\", df_reduce[\"rt_status_user_friends_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_user_statuses_count\", df_reduce[\"rt_status_user_statuses_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_retweet_count\", df_reduce[\"rt_status_retweet_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_user_listed_count\", df_reduce[\"rt_status_user_listed_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_favorite_count\", df_reduce[\"rt_status_favorite_count\"].cast(IntegerType()))\n",
    "df_reduce = df_reduce.withColumn(\"rt_status_num_user_mentions\", df_reduce[\"rt_status_num_user_mentions\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ratio_followers_following\n",
    "# df_reduce = df_reduce.withColumn('ratio_followers_following',df_reduce.rt_status_user_followers_count/(df_reduce.rt_status_user_friends_count+1)+10)\n",
    "# max_ratio_followers_following = df_reduce.select('ratio_followers_following').rdd.flatMap(list).max()\n",
    "# df_reduce = df_reduce.withColumn('ratio_followers_following',df_reduce.ratio_followers_following/max_ratio_followers_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# followers_count rescale\n",
    "max_user_followers_count = df_reduce.select('rt_status_user_followers_count').rdd.flatMap(list).max()\n",
    "df_reduce = df_reduce.withColumn('rescale_rt_status_user_followers_count',df_reduce.rt_status_user_followers_count/max_user_followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listed_count rescale\n",
    "max_listed_count = df_reduce.select('rt_status_user_listed_count').rdd.flatMap(list).max()\n",
    "df_reduce = df_reduce.withColumn('rescale_rt_status_user_listed_count',df_reduce.rt_status_user_listed_count/max_listed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_tweets rescale\n",
    "max_user_statuses_count  = df_reduce.select('rt_status_user_statuses_count').rdd.flatMap(list).max()\n",
    "df_reduce = df_reduce.withColumn('rescale_rt_status_user_statuses_count',df_reduce.rt_status_user_statuses_count/max_user_statuses_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweets_received rescale\n",
    "max_retweets_received  = df_reduce.select('rt_status_retweet_count').rdd.flatMap(list).max()\n",
    "df_reduce = df_reduce.withColumn('rescale_rt_status_retweet_count',df_reduce.rt_status_retweet_count/max_retweets_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweets_received rescale\n",
    "max_favorite_count  = df_reduce.select('rt_status_favorite_count').rdd.flatMap(list).max()\n",
    "df_reduce = df_reduce.withColumn('rescale_rt_status_favorite_count',df_reduce.rt_status_favorite_count/max_favorite_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Influence Score\n",
    "\n",
    "df_reduce=df_reduce.withColumn('Influencer_Score',\n",
    "                              (df_reduce.rescale_rt_status_retweet_count *10 + \n",
    "                              df_reduce.rescale_rt_status_favorite_count * 9 +\n",
    "                              df_reduce.rescale_rt_status_user_followers_count *6 + \n",
    "                              df_reduce.rescale_rt_status_user_statuses_count * 5 +\n",
    "                              df_reduce.rescale_rt_status_user_listed_count* 4 # +\n",
    "#                               df_reduce.ratio_followers_following * 3\n",
    "                              )/34*10 \n",
    "                              )\n",
    "\n",
    "# max_Influencer_Score =  df_reduce.select('Influencer_Score').rdd.flatMap(list).max()\n",
    "# df_reduce = df_reduce.withColumn('Influencer_Score',df_reduce.Influencer_Score/max_Influencer_Score*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_reduce.groupBy('rt_status_user_id').agg(\n",
    "  round(max('Influencer_Score'),2).alias('max_influence_score'),\n",
    "  max('rt_status_retweet_count').alias('max_retweet_count'),\n",
    "  max('rt_status_user_followers_count').alias('max_follower_count'),\n",
    "  max('rt_status_user_friends_count').alias('max_friends_count'),\n",
    "  round(avg('rt_status_sentScore'),2).alias('avg_sentiment'),\n",
    "  round(avg('rt_status_sentMag'),2).alias('avg_sentMag'),\n",
    "  first('rt_status_text').alias('status_text'),\n",
    "  first('rt_status_user_name').alias('user_name'),\n",
    "  first('rt_status_user_profile_image').alias('profile_image'),\n",
    "  first('searched_names').alias('search_name'),).sort('max_influence_score',ascending=False)\n",
    "# .show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_class(value1):\n",
    "  if value1 >-0.4 and value1 <0.4: return 'Neutral'\n",
    "  elif value1 <= -0.4: return 'Negative'\n",
    "  elif value1 >= 0.4 : return 'Positive'\n",
    "\n",
    "df_reduce_udf = udf(sent_class, StringType())  \n",
    "df_final = df_final.withColumn('sent_class',df_reduce_udf('avg_sentiment'))  \n",
    "\n",
    "# df_with_cat = df.withColumn(\"category\", udfValueToCategory(\"x1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView(\"df_final\")\n",
    "neg_influence=spark.sql(\"SELECT * FROM df_final WHERE df_final.sent_class='Negative' LIMIT 5\")\n",
    "pos_influence=spark.sql(\"SELECT * FROM df_final WHERE df_final.sent_class='Positive' LIMIT 5\")\n",
    "neu_influence=spark.sql(\"SELECT * FROM df_final WHERE df_final.sent_class='Neutral' LIMIT 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_influence.write.saveAsTable('neg_influence',mode='overwrite')\n",
    "pos_influence.write.saveAsTable('pos_influence',mode='overwrite')\n",
    "neu_influence.write.saveAsTable('neu_influence',mode='overwrite')\n",
    "\n",
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_text=spark.sql(\"SELECT df_final.status_text FROM df_final WHERE df_final.sent_class='Negative'\")\n",
    "pos_text=spark.sql(\"SELECT df_final.status_text FROM df_final WHERE df_final.sent_class='Positive'\")\n",
    "neu_text=spark.sql(\"SELECT df_final.status_text FROM df_final WHERE df_final.sent_class='Neutral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\",'i','&amp;','-','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(s): return s.split()\n",
    "\n",
    "def stopword(i):\n",
    "  if i in stopwords:\n",
    "    return ''\n",
    "  else:\n",
    "    return i\n",
    "\n",
    "df_words_udf = udf(stopword, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_wordsDF=neg_text.rdd.map(lambda x: x[0])\n",
    "neg_words_rdd = neg_wordsDF.flatMap(split_line)\n",
    "neg_wordcount = neg_words_rdd.map(lambda word: (word.lower(), 1))\n",
    "neg_wc = neg_wordcount.reduceByKey(lambda a, b: a+b)\n",
    "neg_wc_df=neg_wc.toDF()\n",
    "neg_wc_df = neg_wc_df.withColumn('_2',neg_wc_df['_2'].cast(IntegerType()))\n",
    "neg_wc_df_stop= neg_wc_df.withColumn('word',df_words_udf('_1'))\n",
    "neg_word_cloud=neg_wc_df_stop.filter(neg_wc_df_stop.word != '').sort('_2',ascending=False).select('word','_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_wordsDF=pos_text.rdd.map(lambda x: x[0])\n",
    "pos_words_rdd = pos_wordsDF.flatMap(split_line)\n",
    "pos_wordcount = pos_words_rdd.map(lambda word: (word.lower(), 1))\n",
    "pos_wc = pos_wordcount.reduceByKey(lambda a, b: a+b)\n",
    "pos_wc_df=pos_wc.toDF()\n",
    "pos_wc_df = pos_wc_df.withColumn('_2',pos_wc_df['_2'].cast(IntegerType()))\n",
    "pos_wc_df_stop= pos_wc_df.withColumn('word',df_words_udf('_1'))\n",
    "pos_word_cloud=pos_wc_df_stop.filter(pos_wc_df_stop.word != '').sort('_2',ascending=False).select('word','_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_wordsDF=neu_text.rdd.map(lambda x: x[0])\n",
    "neu_words_rdd = neu_wordsDF.flatMap(split_line)\n",
    "neu_wordcount = neu_words_rdd.map(lambda word: (word.lower(), 1))\n",
    "neu_wc = neu_wordcount.reduceByKey(lambda a, b: a+b)\n",
    "neu_wc_df=neu_wc.toDF()\n",
    "neu_wc_df = neu_wc_df.withColumn('_2',neu_wc_df['_2'].cast(IntegerType()))\n",
    "neu_wc_df_stop= neu_wc_df.withColumn('word',df_words_udf('_1'))\n",
    "neu_word_cloud=neu_wc_df_stop.filter(neu_wc_df_stop.word != '').sort('_2',ascending=False).select('word','_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word_cloud.createOrReplaceTempView(\"temp_neg\")\n",
    "pos_word_cloud.createOrReplaceTempView(\"temp_pos\")\n",
    "neu_word_cloud.createOrReplaceTempView(\"temp_neu\")\n",
    "\n",
    "neg_word_cloud=spark.sql(\"SELECT * FROM temp_neg LIMIT 20\")\n",
    "pos_word_cloud=spark.sql(\"SELECT * FROM temp_pos LIMIT 20\")\n",
    "neu_word_cloud=spark.sql(\"SELECT * FROM temp_neu LIMIT 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_word_cloud.write.saveAsTable('neu_word_cloud', mode = 'overwrite')\n",
    "pos_word_cloud.write.saveAsTable('pos_word_cloud', mode = 'overwrite')\n",
    "neg_word_cloud.write.saveAsTable('neg_word_cloud', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "TwitterAnalysis",
  "notebookId": 4.353267888137912E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}